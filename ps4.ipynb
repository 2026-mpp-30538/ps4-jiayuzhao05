{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9952992b",
      "metadata": {},
      "source": [
        "# Step 1: Develop Initial Scraper and Crawler\n",
        "\n",
        "## Scraping HHS OIG Enforcement Actions\n",
        "\n",
        "Scrape the first page of HHS OIG Enforcement Actions and collect:\n",
        "- Title\n",
        "- Date\n",
        "- Category\n",
        "- Link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4223a173",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bd05eaed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status code: 200\n"
          ]
        }
      ],
      "source": [
        "# Set target URL\n",
        "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "\n",
        "# Send HTTP request\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "print(f\"Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "94ab252c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for 'article' tags: 0\n",
            "Looking for 'h2' tags: 24\n",
            "Looking for 'h3' tags: 1\n",
            "Looking for divs with relevant classes: 0\n",
            "\n",
            "First h2 found: In This Section\n",
            "Parent of h2: aside, class: ['block-highlight', 'padding-top-2px', 'margin-bottom-4']\n",
            "Found 24 items based on h2 tags\n",
            "Total enforcement action entries to process: 24\n"
          ]
        }
      ],
      "source": [
        "# Parse HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# what structure actually exists\n",
        "print(f\"Looking for 'article' tags: {len(soup.find_all('article'))}\")\n",
        "print(f\"Looking for 'h2' tags: {len(soup.find_all('h2'))}\")\n",
        "print(f\"Looking for 'h3' tags: {len(soup.find_all('h3'))}\")\n",
        "\n",
        "# Try different container types\n",
        "import re\n",
        "divs = soup.find_all('div', class_=re.compile('views-row|item|enforcement'))\n",
        "print(f\"Looking for divs with relevant classes: {len(divs)}\")\n",
        "\n",
        "# Show first h2 if exists\n",
        "first_h2 = soup.find('h2')\n",
        "if first_h2:\n",
        "    print(f\"\\nFirst h2 found: {first_h2.get_text(strip=True)[:100]}\")\n",
        "    print(f\"Parent of h2: {first_h2.parent.name}, class: {first_h2.parent.get('class')}\")\n",
        "\n",
        "# Find all enforcement action entries\n",
        "enforcement_actions = []\n",
        "\n",
        "# Strategy: Look for h2 or h3 headings which likely contain titles\n",
        "# Find their parent containers\n",
        "articles = []\n",
        "\n",
        "# Try to find the main content area first\n",
        "main_content = soup.find('div', class_='views-view-content')\n",
        "if main_content:\n",
        "    # Look for direct children that might be rows\n",
        "    articles = main_content.find_all('div', recursive=False)\n",
        "    print(f\"Found {len(articles)} items in main content area\")\n",
        "else:\n",
        "    # Fallback: look for h2 tags and use their parents\n",
        "    h2_tags = soup.find_all('h2')\n",
        "    articles = [h2.parent for h2 in h2_tags if h2.parent]\n",
        "    print(f\"Found {len(articles)} items based on h2 tags\")\n",
        "\n",
        "print(f\"Total enforcement action entries to process: {len(articles)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dc6ffd3c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully extracted 24 records\n",
            "\n",
            "First 3 records:\n",
            "\n",
            "1. Title: In This Section...\n",
            "   Date: None\n",
            "   Category: About Enforcement Actions, Civil Monetary Penalty Authorities\n",
            "\n",
            "2. Title: Archives...\n",
            "   Date: None\n",
            "   Category: None\n",
            "\n",
            "3. Title: Brooklyn Banker Pleads Guilty to Laundering Proceeds of Medicare Fraud for Trans...\n",
            "   Date: February 3, 2026\n",
            "   Category: COVID-19\n"
          ]
        }
      ],
      "source": [
        "# Extract information for each enforcement action\n",
        "for idx, article in enumerate(articles):\n",
        "    # Extract title and link - try multiple methods\n",
        "    title = None\n",
        "    link = None\n",
        "    \n",
        "    # Method 1: Look for h2 or h3 with link\n",
        "    heading = article.find(['h2', 'h3'])\n",
        "    if heading:\n",
        "        link_element = heading.find('a')\n",
        "        if link_element:\n",
        "            title = link_element.get_text(strip=True)\n",
        "            link = link_element.get('href')\n",
        "            # If link is relative path, convert to absolute path\n",
        "            if link and not link.startswith('http'):\n",
        "                link = 'https://oig.hhs.gov' + link\n",
        "        else:\n",
        "            title = heading.get_text(strip=True)\n",
        "    \n",
        "    # Method 2: If no heading, look for any link\n",
        "    if not title:\n",
        "        link_element = article.find('a')\n",
        "        if link_element:\n",
        "            title = link_element.get_text(strip=True)\n",
        "            link = link_element.get('href')\n",
        "            if link and not link.startswith('http'):\n",
        "                link = 'https://oig.hhs.gov' + link\n",
        "    \n",
        "    # Skip if no title found\n",
        "    if not title:\n",
        "        continue\n",
        "    \n",
        "    # Extract date\n",
        "    date = None\n",
        "    # Method 1: Look for time tag\n",
        "    date_element = article.find('time')\n",
        "    if date_element:\n",
        "        date = date_element.get_text(strip=True)\n",
        "    else:\n",
        "        # Method 2: Look for date pattern in text\n",
        "        text = article.get_text()\n",
        "        date_pattern = r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}'\n",
        "        match = re.search(date_pattern, text)\n",
        "        if match:\n",
        "            date = match.group(0)\n",
        "    \n",
        "    # Extract category\n",
        "    categories = []\n",
        "    category_elements = article.find_all('li')\n",
        "    for cat in category_elements:\n",
        "        cat_text = cat.get_text(strip=True)\n",
        "        if cat_text:\n",
        "            categories.append(cat_text)\n",
        "    category = ', '.join(categories) if categories else None\n",
        "    \n",
        "    # Add to list\n",
        "    enforcement_actions.append({\n",
        "        'title': title,\n",
        "        'date': date,\n",
        "        'category': category,\n",
        "        'link': link\n",
        "    })\n",
        "\n",
        "print(f\"\\nSuccessfully extracted {len(enforcement_actions)} records\")\n",
        "\n",
        "# Show first few records for verification\n",
        "if enforcement_actions:\n",
        "    print(\"\\nFirst 3 records:\")\n",
        "    for i, action in enumerate(enforcement_actions[:3]):\n",
        "        print(f\"\\n{i+1}. Title: {action['title'][:80]}...\")\n",
        "        print(f\"   Date: {action['date']}\")\n",
        "        print(f\"   Category: {action['category']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5f787bbb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top rows in dataset:\n",
            "                                                                                                            title  \\\n",
            "0                                                                                                 In This Section   \n",
            "1                                                                                                        Archives   \n",
            "2  Brooklyn Banker Pleads Guilty to Laundering Proceeds of Medicare Fraud for Transnational Criminal Organization   \n",
            "3                  Delafield Man Sentenced to 18 Months’ Imprisonment for Conspiracy to Pay Health Care Kickbacks   \n",
            "4                                                            Former NFL Player Convicted for $197M Medicare Fraud   \n",
            "\n",
            "               date  \\\n",
            "0              None   \n",
            "1              None   \n",
            "2  February 3, 2026   \n",
            "3  February 3, 2026   \n",
            "4  February 3, 2026   \n",
            "\n",
            "                                                        category  \\\n",
            "0  About Enforcement Actions, Civil Monetary Penalty Authorities   \n",
            "1                                                           None   \n",
            "2                                                       COVID-19   \n",
            "3                                     Criminal and Civil Actions   \n",
            "4                                     Criminal and Civil Actions   \n",
            "\n",
            "                                                                                                                                                    link  \n",
            "0                                                                                                                                                   None  \n",
            "1                                                                                                                                                   None  \n",
            "2  https://oig.hhs.gov/fraud/enforcement/brooklyn-banker-pleads-guilty-to-laundering-proceeds-of-medicare-fraud-for-transnational-criminal-organization/  \n",
            "3                   https://oig.hhs.gov/fraud/enforcement/delafield-man-sentenced-to-18-months-imprisonment-for-conspiracy-to-pay-health-care-kickbacks/  \n",
            "4                                                             https://oig.hhs.gov/fraud/enforcement/former-nfl-player-convicted-for-197m-medicare-fraud/  \n"
          ]
        }
      ],
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(enforcement_actions)\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Top rows in dataset:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a1f55cdb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset shape: (24, 4)\n",
            "\n",
            "Columns: ['title', 'date', 'category', 'link']\n",
            "\n",
            "Data types:\n",
            "title       object\n",
            "date        object\n",
            "category    object\n",
            "link        object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# View basic information of the dataframe\n",
        "print(\"\\nDataset shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "51bc392e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 records:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>category</th>\n",
              "      <th>link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In This Section</td>\n",
              "      <td>None</td>\n",
              "      <td>About Enforcement Actions, Civil Monetary Penalty Authorities</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Archives</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Brooklyn Banker Pleads Guilty to Laundering Proceeds of Medicare Fraud for Transnational Criminal Organization</td>\n",
              "      <td>February 3, 2026</td>\n",
              "      <td>COVID-19</td>\n",
              "      <td>https://oig.hhs.gov/fraud/enforcement/brooklyn-banker-pleads-guilty-to-laundering-proceeds-of-medicare-fraud-for-transnational-criminal-organization/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Delafield Man Sentenced to 18 Months’ Imprisonment for Conspiracy to Pay Health Care Kickbacks</td>\n",
              "      <td>February 3, 2026</td>\n",
              "      <td>Criminal and Civil Actions</td>\n",
              "      <td>https://oig.hhs.gov/fraud/enforcement/delafield-man-sentenced-to-18-months-imprisonment-for-conspiracy-to-pay-health-care-kickbacks/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Former NFL Player Convicted for $197M Medicare Fraud</td>\n",
              "      <td>February 3, 2026</td>\n",
              "      <td>Criminal and Civil Actions</td>\n",
              "      <td>https://oig.hhs.gov/fraud/enforcement/former-nfl-player-convicted-for-197m-medicare-fraud/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AG's Office Secures Indictments Against Peabody Alcohol and Drug Counselor and Her Businesses for More Than $850,000 in MassHealth Fraud</td>\n",
              "      <td>February 2, 2026</td>\n",
              "      <td>State Enforcement Agencies</td>\n",
              "      <td>https://oig.hhs.gov/fraud/enforcement/ags-office-secures-indictments-against-peabody-alcohol-and-drug-counselor-and-her-businesses-for-more-than-850000-in-masshealth-fraud/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Florida Man Pleads Guilty to Conspiracy to Violate the Anti-Kickback Statute</td>\n",
              "      <td>January 30, 2026</td>\n",
              "      <td>Criminal and Civil Actions</td>\n",
              "      <td>https://oig.hhs.gov/fraud/enforcement/florida-man-pleads-guilty-to-conspiracy-to-violate-the-anti-kickback-statute/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Yadkinville Woman Sentenced in Connection with Multi-Million Dollar Medicaid Fraud Scheme</td>\n",
              "      <td>January 29, 2026</td>\n",
              "      <td>Criminal and Civil Actions</td>\n",
              "      <td>https://oig.hhs.gov/fraud/enforcement/yadkinville-woman-sentenced-in-connection-with-multi-million-dollar-medicaid-fraud-scheme/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Slidell Chiropractor Sentenced for Health Care Fraud</td>\n",
              "      <td>January 28, 2026</td>\n",
              "      <td>COVID-19, Criminal and Civil Actions</td>\n",
              "      <td>https://oig.hhs.gov/fraud/enforcement/slidell-chiropractor-sentenced-for-health-care-fraud/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Repeat Health Care Fraud Offender Sentenced for Defrauding New Hampshire Medicaid</td>\n",
              "      <td>January 28, 2026</td>\n",
              "      <td>Criminal and Civil Actions</td>\n",
              "      <td>https://oig.hhs.gov/fraud/enforcement/repeat-health-care-fraud-offender-sentenced-for-defrauding-new-hampshire-medicaid/</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                      title  \\\n",
              "0                                                                                                                           In This Section   \n",
              "1                                                                                                                                  Archives   \n",
              "2                            Brooklyn Banker Pleads Guilty to Laundering Proceeds of Medicare Fraud for Transnational Criminal Organization   \n",
              "3                                            Delafield Man Sentenced to 18 Months’ Imprisonment for Conspiracy to Pay Health Care Kickbacks   \n",
              "4                                                                                      Former NFL Player Convicted for $197M Medicare Fraud   \n",
              "5  AG's Office Secures Indictments Against Peabody Alcohol and Drug Counselor and Her Businesses for More Than $850,000 in MassHealth Fraud   \n",
              "6                                                              Florida Man Pleads Guilty to Conspiracy to Violate the Anti-Kickback Statute   \n",
              "7                                                 Yadkinville Woman Sentenced in Connection with Multi-Million Dollar Medicaid Fraud Scheme   \n",
              "8                                                                                      Slidell Chiropractor Sentenced for Health Care Fraud   \n",
              "9                                                         Repeat Health Care Fraud Offender Sentenced for Defrauding New Hampshire Medicaid   \n",
              "\n",
              "               date  \\\n",
              "0              None   \n",
              "1              None   \n",
              "2  February 3, 2026   \n",
              "3  February 3, 2026   \n",
              "4  February 3, 2026   \n",
              "5  February 2, 2026   \n",
              "6  January 30, 2026   \n",
              "7  January 29, 2026   \n",
              "8  January 28, 2026   \n",
              "9  January 28, 2026   \n",
              "\n",
              "                                                        category  \\\n",
              "0  About Enforcement Actions, Civil Monetary Penalty Authorities   \n",
              "1                                                           None   \n",
              "2                                                       COVID-19   \n",
              "3                                     Criminal and Civil Actions   \n",
              "4                                     Criminal and Civil Actions   \n",
              "5                                     State Enforcement Agencies   \n",
              "6                                     Criminal and Civil Actions   \n",
              "7                                     Criminal and Civil Actions   \n",
              "8                           COVID-19, Criminal and Civil Actions   \n",
              "9                                     Criminal and Civil Actions   \n",
              "\n",
              "                                                                                                                                                                           link  \n",
              "0                                                                                                                                                                          None  \n",
              "1                                                                                                                                                                          None  \n",
              "2                         https://oig.hhs.gov/fraud/enforcement/brooklyn-banker-pleads-guilty-to-laundering-proceeds-of-medicare-fraud-for-transnational-criminal-organization/  \n",
              "3                                          https://oig.hhs.gov/fraud/enforcement/delafield-man-sentenced-to-18-months-imprisonment-for-conspiracy-to-pay-health-care-kickbacks/  \n",
              "4                                                                                    https://oig.hhs.gov/fraud/enforcement/former-nfl-player-convicted-for-197m-medicare-fraud/  \n",
              "5  https://oig.hhs.gov/fraud/enforcement/ags-office-secures-indictments-against-peabody-alcohol-and-drug-counselor-and-her-businesses-for-more-than-850000-in-masshealth-fraud/  \n",
              "6                                                           https://oig.hhs.gov/fraud/enforcement/florida-man-pleads-guilty-to-conspiracy-to-violate-the-anti-kickback-statute/  \n",
              "7                                              https://oig.hhs.gov/fraud/enforcement/yadkinville-woman-sentenced-in-connection-with-multi-million-dollar-medicaid-fraud-scheme/  \n",
              "8                                                                                   https://oig.hhs.gov/fraud/enforcement/slidell-chiropractor-sentenced-for-health-care-fraud/  \n",
              "9                                                      https://oig.hhs.gov/fraud/enforcement/repeat-health-care-fraud-offender-sentenced-for-defrauding-new-hampshire-medicaid/  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display complete first 10 records\n",
        "print(\"\\nTop 10 records:\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "882ca902",
      "metadata": {},
      "source": [
        "## ⚠️ CHECKPOINT: Verify Step 1 Works Before Continuing\n",
        "\n",
        "Before proceeding to Step 2, make sure the cells above extracted data successfully. \n",
        "You should have seen output like:\n",
        "- \"Found X items in main content area\" (where X > 0)\n",
        "- \"Successfully extracted X records\" (where X > 0)\n",
        "- Sample records displayed\n",
        "\n",
        "If you got 0 records, the scraping function in Step 2 will also fail."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ea9737",
      "metadata": {},
      "source": [
        "# Step 2: Making the Scraper Dynamic\n",
        "\n",
        "## Creating a Function to Scrape Enforcement Actions by Date Range\n",
        "\n",
        "This function will:\n",
        "- Take a month and year as input\n",
        "- Scrape all enforcement actions from that date to today\n",
        "- Save results to a CSV file\n",
        "- Include a run indicator to control execution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e498e80",
      "metadata": {},
      "source": [
        "### Part a: Pseudo-code for Dynamic Scraper Function\n",
        "\n",
        "**Function:** `scrape_enforcement_actions(month, year, run=False)`\n",
        "\n",
        "**Steps:**\n",
        "1. **Input validation:**\n",
        "   - Check if year >= 2013 (data only available from 2013 onwards)\n",
        "   - If year < 2013, print warning message and return None\n",
        "   - If run=False, print message and return None without scraping\n",
        "\n",
        "2. **Initialize variables:**\n",
        "   - Create empty list to store all enforcement actions\n",
        "   - Set base URL for enforcement actions page\n",
        "   - Set up headers for HTTP requests\n",
        "   - Calculate target start date from month/year input\n",
        "\n",
        "3. **Loop through pages (using a while loop):**\n",
        "   - Why while loop? We don't know how many pages we need in advance - we need to keep scraping until we reach the target date or run out of pages\n",
        "   - Start with page 1\n",
        "   - For each page:\n",
        "     - Make HTTP request to current page URL\n",
        "     - Add 1 second delay to avoid server blocking\n",
        "     - Parse HTML content\n",
        "     - Extract all enforcement action articles\n",
        "     - For each article:\n",
        "       - Extract title, date, category, and link\n",
        "       - Parse the date and compare with target start date\n",
        "       - If date >= target date, add to list\n",
        "       - If date < target date, set flag to stop pagination\n",
        "     - Check if there's a \"Next\" button/link for pagination\n",
        "     - If no more pages or reached target date, exit loop\n",
        "     - Otherwise, increment page number and continue\n",
        "\n",
        "4. **Process results:**\n",
        "   - Convert list to pandas DataFrame\n",
        "   - Create filename: \"enforcement_actions_{year}_{month}.csv\"\n",
        "   - Save DataFrame to CSV file\n",
        "   - Print summary statistics (total records, date range)\n",
        "   - Return DataFrame\n",
        "\n",
        "**Loop type rationale:** \n",
        "- While loop is most appropriate because:\n",
        "  - We don't know the total number of pages beforehand\n",
        "  - We need to check conditions (date range, page availability) on each iteration\n",
        "  - We may need to stop early if we reach our target date\n",
        "  - More flexible than for loop with range()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8d89e821",
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'dateutil' from 'dateutil.relativedelta' (c:\\ProgramData\\anaconda3\\Lib\\site-packages\\dateutil\\relativedelta.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import additional libraries for date handling\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdateutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelativedelta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dateutil\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'dateutil' from 'dateutil.relativedelta' (c:\\ProgramData\\anaconda3\\Lib\\site-packages\\dateutil\\relativedelta.py)"
          ]
        }
      ],
      "source": [
        "# Import additional libraries for date handling\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Note: We don't need dateutil for this project\n",
        "# All date parsing can be done with datetime.strptime()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b6d39001",
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_enforcement_actions(month, year, run=False):\n",
        "    \"\"\"\n",
        "    Scrape HHS OIG enforcement actions from a given month/year to today.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    month : int\n",
        "        Starting month (1-12)\n",
        "    year : int\n",
        "        Starting year (must be >= 2013)\n",
        "    run : bool\n",
        "        If False, function will not execute (default: False)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame or None\n",
        "        DataFrame containing scraped enforcement actions, or None if not run\n",
        "    \"\"\"\n",
        "    \n",
        "    # Check if function should run\n",
        "    if not run:\n",
        "        print(\"Function not executed. Set run=True to scrape data.\")\n",
        "        return None\n",
        "    \n",
        "    # Validate year\n",
        "    if year < 2013:\n",
        "        print(f\"Error: Year must be >= 2013. Only enforcement actions after 2013 are listed.\")\n",
        "        print(f\"You entered: {year}\")\n",
        "        return None\n",
        "    \n",
        "    # Validate month\n",
        "    if month < 1 or month > 12:\n",
        "        print(f\"Error: Month must be between 1 and 12. You entered: {month}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"Starting to scrape enforcement actions from {month}/{year} to today...\")\n",
        "    \n",
        "    # Create target start date\n",
        "    target_date = datetime(year, month, 1)\n",
        "    \n",
        "    # Initialize variables\n",
        "    all_enforcement_actions = []\n",
        "    base_url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    \n",
        "    page_num = 0\n",
        "    should_continue = True\n",
        "    \n",
        "    # Loop through pages until we reach target date or run out of pages\n",
        "    while should_continue:\n",
        "        # Construct URL for current page\n",
        "        if page_num == 0:\n",
        "            url = base_url\n",
        "        else:\n",
        "            url = f\"{base_url}?page={page_num}\"\n",
        "        \n",
        "        print(f\"Scraping page {page_num + 1}... (URL: {url})\")\n",
        "        \n",
        "        # Make HTTP request\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching page {page_num + 1}: {e}\")\n",
        "            break\n",
        "        \n",
        "        # Parse HTML\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        # Find all enforcement action articles\n",
        "        articles = soup.find_all('article')\n",
        "        \n",
        "        if not articles:\n",
        "            print(f\"No more articles found on page {page_num + 1}. Stopping.\")\n",
        "            break\n",
        "        \n",
        "        print(f\"  Found {len(articles)} articles on this page\")\n",
        "        \n",
        "        # Track if we found any articles within date range on this page\n",
        "        found_in_range = False\n",
        "        \n",
        "        # Extract information from each article\n",
        "        for article in articles:\n",
        "            # Extract title and link\n",
        "            title_element = article.find('h2')\n",
        "            if title_element:\n",
        "                link_element = title_element.find('a')\n",
        "                if link_element:\n",
        "                    title = link_element.get_text(strip=True)\n",
        "                    link = link_element.get('href')\n",
        "                    if link and not link.startswith('http'):\n",
        "                        link = 'https://oig.hhs.gov' + link\n",
        "                else:\n",
        "                    title = title_element.get_text(strip=True)\n",
        "                    link = None\n",
        "            else:\n",
        "                title = None\n",
        "                link = None\n",
        "            \n",
        "            # Extract date\n",
        "            date_element = article.find('time')\n",
        "            if date_element:\n",
        "                date_str = date_element.get_text(strip=True)\n",
        "                \n",
        "                # Parse date to compare with target\n",
        "                try:\n",
        "                    # Try parsing common date formats\n",
        "                    article_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
        "                except:\n",
        "                    try:\n",
        "                        article_date = datetime.strptime(date_str, \"%B %Y\")\n",
        "                    except:\n",
        "                        # If parsing fails, include the article anyway\n",
        "                        article_date = target_date\n",
        "            else:\n",
        "                date_str = None\n",
        "                article_date = target_date\n",
        "            \n",
        "            # Check if article is within our date range\n",
        "            if article_date >= target_date:\n",
        "                found_in_range = True\n",
        "                \n",
        "                # Extract categories\n",
        "                categories = []\n",
        "                category_elements = article.find_all('li')\n",
        "                for cat in category_elements:\n",
        "                    categories.append(cat.get_text(strip=True))\n",
        "                category = ', '.join(categories) if categories else None\n",
        "                \n",
        "                # Add to list\n",
        "                all_enforcement_actions.append({\n",
        "                    'title': title,\n",
        "                    'date': date_str,\n",
        "                    'category': category,\n",
        "                    'link': link\n",
        "                })\n",
        "            else:\n",
        "                # If we've gone past our target date, we can stop\n",
        "                print(f\"  Reached articles before target date ({date_str}). Stopping.\")\n",
        "                should_continue = False\n",
        "                break\n",
        "        \n",
        "        # Check for next page\n",
        "        # Look for pagination links\n",
        "        pagination = soup.find('nav', {'aria-label': 'Pagination'})\n",
        "        if pagination:\n",
        "            next_link = pagination.find('a', text=re.compile(r'Next|›'))\n",
        "            if not next_link or 'disabled' in next_link.get('class', []):\n",
        "                print(\"  No next page available. Stopping.\")\n",
        "                should_continue = False\n",
        "        else:\n",
        "            # If no pagination found, try to detect if there are more pages\n",
        "            # by checking if we got a full page of results\n",
        "            if len(articles) < 20:  # Assuming 20 items per page\n",
        "                should_continue = False\n",
        "        \n",
        "        # Increment page number\n",
        "        page_num += 1\n",
        "        \n",
        "        # Add delay to avoid overwhelming the server\n",
        "        if should_continue:\n",
        "            time.sleep(1)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    if all_enforcement_actions:\n",
        "        df = pd.DataFrame(all_enforcement_actions)\n",
        "        \n",
        "        # Save to CSV\n",
        "        filename = f\"enforcement_actions_{year}_{month}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Scraping complete!\")\n",
        "        print(f\"Total enforcement actions collected: {len(df)}\")\n",
        "        print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "        print(f\"Saved to: {filename}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        return df\n",
        "    else:\n",
        "        print(\"No enforcement actions found in the specified date range.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed471b7e",
      "metadata": {},
      "source": [
        "### Part b: Run the Dynamic Scraper\n",
        "\n",
        "Test the function by scraping enforcement actions since January 2024."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bcfdd138",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function not executed. Set run=True to scrape data.\n"
          ]
        }
      ],
      "source": [
        "# Test with run=False first (should not execute)\n",
        "test_df = scrape_enforcement_actions(month=1, year=2024, run=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "52c5af5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting to scrape enforcement actions from 1/2024 to today...\n",
            "Scraping page 1... (URL: https://oig.hhs.gov/fraud/enforcement/)\n",
            "No more articles found on page 1. Stopping.\n",
            "No enforcement actions found in the specified date range.\n"
          ]
        }
      ],
      "source": [
        "# Now run with run=True to actually scrape data from January 2024\n",
        "df_2024 = scrape_enforcement_actions(month=1, year=2024, run=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0bd72e3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the results\n",
        "if df_2024 is not None:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ANALYSIS OF SCRAPED DATA\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Total number of enforcement actions\n",
        "    total_actions = len(df_2024)\n",
        "    print(f\"\\n1. Total enforcement actions collected: {total_actions}\")\n",
        "    \n",
        "    # Display first few rows\n",
        "    print(f\"\\n2. First few records:\")\n",
        "    print(df_2024.head())\n",
        "    \n",
        "    # Find the earliest enforcement action\n",
        "    print(f\"\\n3. Earliest Enforcement Action Details:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Get the last row (assuming data is sorted by date descending)\n",
        "    earliest_action = df_2024.iloc[-1]\n",
        "    \n",
        "    print(f\"Date: {earliest_action['date']}\")\n",
        "    print(f\"Title: {earliest_action['title']}\")\n",
        "    print(f\"Category: {earliest_action['category']}\")\n",
        "    print(f\"Link: {earliest_action['link']}\")\n",
        "    \n",
        "    # Also show the most recent action\n",
        "    print(f\"\\n4. Most Recent Enforcement Action Details:\")\n",
        "    print(\"-\" * 60)\n",
        "    most_recent = df_2024.iloc[0]\n",
        "    print(f\"Date: {most_recent['date']}\")\n",
        "    print(f\"Title: {most_recent['title']}\")\n",
        "    print(f\"Category: {most_recent['category']}\")\n",
        "    print(f\"Link: {most_recent['link']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9034a205",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display DataFrame info and sample records\n",
        "if df_2024 is not None:\n",
        "    print(\"\\nDataFrame Information:\")\n",
        "    print(df_2024.info())\n",
        "    \n",
        "    print(\"\\n\\nSample of 10 records:\")\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.max_colwidth', 50)\n",
        "    display(df_2024.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8f3503",
      "metadata": {},
      "source": [
        "## Debugging: Check the actual HTML structure\n",
        "\n",
        "Let's inspect the webpage to understand its structure better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "05522268",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status code: 200\n",
            "Response length: 72654 bytes\n",
            "\n",
            "--- Checking for 'article' tags ---\n",
            "Found 0 article tags\n",
            "\n",
            "--- Checking for other possible containers ---\n",
            "Found 20 divs with relevant classes\n",
            "Found 80 list items with relevant classes\n",
            "Found 25 h2/h3 headings\n",
            "\n",
            "First 3 headings:\n",
            "1. In This Section\n",
            "   Parent: aside, Parent class: ['block-highlight', 'padding-top-2px', 'margin-bottom-4']\n",
            "\n",
            "2. Filters\n",
            "   Parent: form, Parent class: ['filtered-list-form', 'input', 'select', 'option']\n",
            "\n",
            "3. Archives\n",
            "   Parent: div, Parent class: ['margin-y-5']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check the HTML structure\n",
        "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "print(f\"Status code: {response.status_code}\")\n",
        "print(f\"Response length: {len(response.content)} bytes\")\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Check for different possible structures\n",
        "print(\"\\n--- Checking for 'article' tags ---\")\n",
        "articles = soup.find_all('article')\n",
        "print(f\"Found {len(articles)} article tags\")\n",
        "\n",
        "print(\"\\n--- Checking for other possible containers ---\")\n",
        "# Check for divs with specific classes\n",
        "divs_with_class = soup.find_all('div', class_=re.compile('item|action|enforcement|card'))\n",
        "print(f\"Found {len(divs_with_class)} divs with relevant classes\")\n",
        "\n",
        "# Check for list items\n",
        "list_items = soup.find_all('li', class_=re.compile('item|action'))\n",
        "print(f\"Found {len(list_items)} list items with relevant classes\")\n",
        "\n",
        "# Look for h2 or h3 headings (titles)\n",
        "headings = soup.find_all(['h2', 'h3'])\n",
        "print(f\"Found {len(headings)} h2/h3 headings\")\n",
        "\n",
        "# Show first few headings\n",
        "if headings:\n",
        "    print(\"\\nFirst 3 headings:\")\n",
        "    for i, h in enumerate(headings[:3]):\n",
        "        print(f\"{i+1}. {h.get_text(strip=True)[:100]}\")\n",
        "        print(f\"   Parent: {h.parent.name}, Parent class: {h.parent.get('class')}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "06baad86",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Looking for main content structure ---\n",
            "\n",
            "\n",
            "--- Checking for table structure ---\n",
            "Found 0 tables\n",
            "\n",
            "--- Checking for views/row pattern ---\n",
            "Found 16 rows with 'views-row' or 'row' class\n",
            "\n",
            "First row structure:\n",
            "Classes: ['grid-row', 'grid-gap-lg']\n",
            "Text (first 200 chars): Official websites use .govA.govwebsite belongs to an official government organization in the United States.Secure .gov websites use HTTPSAlock(LockA locked padlock) orhttps://means you’ve safely conne\n"
          ]
        }
      ],
      "source": [
        "# look for the main content area and see what structure contains the enforcement actions\n",
        "print(\"--- Looking for main content structure ---\\n\")\n",
        "\n",
        "# Try to find the main results container\n",
        "results_div = soup.find('div', class_='views-view-content')\n",
        "if results_div:\n",
        "    print(\"Found 'views-view-content' div\")\n",
        "    children = results_div.find_all(recursive=False)\n",
        "    print(f\"Direct children: {len(children)}\")\n",
        "    if children:\n",
        "        print(f\"First child tag: {children[0].name}, class: {children[0].get('class')}\")\n",
        "        \n",
        "# Look for specific patterns in the HTML\n",
        "print(\"\\n--- Checking for table structure ---\")\n",
        "tables = soup.find_all('table')\n",
        "print(f\"Found {len(tables)} tables\")\n",
        "\n",
        "# Check for a views row pattern (common in Drupal sites)\n",
        "print(\"\\n--- Checking for views/row pattern ---\")\n",
        "rows = soup.find_all('div', class_=re.compile('views-row|row'))\n",
        "print(f\"Found {len(rows)} rows with 'views-row' or 'row' class\")\n",
        "\n",
        "if rows:\n",
        "    print(\"\\nFirst row structure:\")\n",
        "    first_row = rows[0]\n",
        "    print(f\"Classes: {first_row.get('class')}\")\n",
        "    print(f\"Text (first 200 chars): {first_row.get_text(strip=True)[:200]}\")\n",
        "    \n",
        "    # Check what's inside the row\n",
        "    title = first_row.find(['h2', 'h3', 'a'])\n",
        "    if title:\n",
        "        print(f\"\\nTitle element: {title.name}, text: {title.get_text(strip=True)[:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af63340",
      "metadata": {},
      "source": [
        "## Updated Scraper Function (More Flexible)\n",
        "\n",
        "Based on debugging, let's create a more robust scraper that can handle different HTML structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7b5825e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_enforcement_actions_v2(month, year, run=False):\n",
        "    \"\"\"\n",
        "    Improved scraper for HHS OIG enforcement actions from a given month/year to today.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    month : int\n",
        "        Starting month (1-12)\n",
        "    year : int\n",
        "        Starting year (must be >= 2013)\n",
        "    run : bool\n",
        "        If False, function will not execute (default: False)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame or None\n",
        "        DataFrame containing scraped enforcement actions, or None if not run\n",
        "    \"\"\"\n",
        "    \n",
        "    # Check if function should run\n",
        "    if not run:\n",
        "        print(\"Function not executed. Set run=True to scrape data.\")\n",
        "        return None\n",
        "    \n",
        "    # Validate year\n",
        "    if year < 2013:\n",
        "        print(f\"Error: Year must be >= 2013. Only enforcement actions after 2013 are listed.\")\n",
        "        print(f\"You entered: {year}\")\n",
        "        return None\n",
        "    \n",
        "    # Validate month\n",
        "    if month < 1 or month > 12:\n",
        "        print(f\"Error: Month must be between 1 and 12. You entered: {month}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"Starting to scrape enforcement actions from {month}/{year} to today...\")\n",
        "    \n",
        "    # Create target start date\n",
        "    target_date = datetime(year, month, 1)\n",
        "    \n",
        "    # Initialize variables\n",
        "    all_enforcement_actions = []\n",
        "    base_url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    \n",
        "    page_num = 0\n",
        "    should_continue = True\n",
        "    \n",
        "    # Loop through pages until we reach target date or run out of pages\n",
        "    while should_continue:\n",
        "        # Construct URL for current page\n",
        "        if page_num == 0:\n",
        "            url = base_url\n",
        "        else:\n",
        "            url = f\"{base_url}?page={page_num}\"\n",
        "        \n",
        "        print(f\"Scraping page {page_num + 1}... (URL: {url})\")\n",
        "        \n",
        "        # Make HTTP request\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching page {page_num + 1}: {e}\")\n",
        "            break\n",
        "        \n",
        "        # Parse HTML\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        # Try multiple strategies to find enforcement actions\n",
        "        items = []\n",
        "        \n",
        "        # Strategy 1: Look for article tags\n",
        "        items = soup.find_all('article')\n",
        "        \n",
        "        # Strategy 2: Look for views-row divs (common in Drupal)\n",
        "        if not items:\n",
        "            items = soup.find_all('div', class_=re.compile('views-row'))\n",
        "        \n",
        "        # Strategy 3: Look for specific list items\n",
        "        if not items:\n",
        "            items = soup.find_all('li', class_=re.compile('enforcement|action'))\n",
        "        \n",
        "        # Strategy 4: Look for divs with h2 headings\n",
        "        if not items:\n",
        "            main_content = soup.find('div', class_='views-view-content')\n",
        "            if main_content:\n",
        "                items = main_content.find_all('div', recursive=False)\n",
        "        \n",
        "        if not items:\n",
        "            print(f\"No items found on page {page_num + 1}. Stopping.\")\n",
        "            break\n",
        "        \n",
        "        print(f\"  Found {len(items)} items on this page\")\n",
        "        \n",
        "        # Track if we found any articles within date range on this page\n",
        "        found_in_range = False\n",
        "        \n",
        "        # Extract information from each item\n",
        "        for item in items:\n",
        "            # Extract title and link - try multiple methods\n",
        "            title = None\n",
        "            link = None\n",
        "            \n",
        "            # Look for h2 or h3 with a link\n",
        "            heading = item.find(['h2', 'h3'])\n",
        "            if heading:\n",
        "                link_element = heading.find('a')\n",
        "                if link_element:\n",
        "                    title = link_element.get_text(strip=True)\n",
        "                    link = link_element.get('href')\n",
        "                    if link and not link.startswith('http'):\n",
        "                        link = 'https://oig.hhs.gov' + link\n",
        "                else:\n",
        "                    title = heading.get_text(strip=True)\n",
        "            \n",
        "            # If no heading found, look for any link\n",
        "            if not title:\n",
        "                link_element = item.find('a')\n",
        "                if link_element:\n",
        "                    title = link_element.get_text(strip=True)\n",
        "                    link = link_element.get('href')\n",
        "                    if link and not link.startswith('http'):\n",
        "                        link = 'https://oig.hhs.gov' + link\n",
        "            \n",
        "            # Skip if no title found\n",
        "            if not title:\n",
        "                continue\n",
        "            \n",
        "            # Extract date - try multiple methods\n",
        "            date_str = None\n",
        "            article_date = None\n",
        "            \n",
        "            # Method 1: Look for time tag\n",
        "            date_element = item.find('time')\n",
        "            if date_element:\n",
        "                date_str = date_element.get_text(strip=True)\n",
        "            \n",
        "            # Method 2: Look for date in specific div/span\n",
        "            if not date_str:\n",
        "                date_div = item.find(['div', 'span'], class_=re.compile('date|time'))\n",
        "                if date_div:\n",
        "                    date_str = date_div.get_text(strip=True)\n",
        "            \n",
        "            # Method 3: Look for date pattern in text\n",
        "            if not date_str:\n",
        "                text = item.get_text()\n",
        "                # Look for common date patterns\n",
        "                date_pattern = r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}'\n",
        "                match = re.search(date_pattern, text)\n",
        "                if match:\n",
        "                    date_str = match.group(0)\n",
        "            \n",
        "            # Parse date if found\n",
        "            if date_str:\n",
        "                try:\n",
        "                    article_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
        "                except:\n",
        "                    try:\n",
        "                        # Try another format\n",
        "                        date_str_clean = re.sub(r'\\s+', ' ', date_str.strip())\n",
        "                        article_date = datetime.strptime(date_str_clean, \"%B %d, %Y\")\n",
        "                    except:\n",
        "                        # If parsing fails, use current date to include the article\n",
        "                        article_date = datetime.now()\n",
        "            else:\n",
        "                # If no date found, use current date to include the article\n",
        "                article_date = datetime.now()\n",
        "            \n",
        "            # Check if article is within our date range\n",
        "            if article_date >= target_date:\n",
        "                found_in_range = True\n",
        "                \n",
        "                # Extract categories\n",
        "                categories = []\n",
        "                # Look for list items that might be categories\n",
        "                category_elements = item.find_all('li')\n",
        "                for cat in category_elements:\n",
        "                    cat_text = cat.get_text(strip=True)\n",
        "                    if cat_text and len(cat_text) > 0:\n",
        "                        categories.append(cat_text)\n",
        "                \n",
        "                # If no categories in list items, look for spans or divs with category class\n",
        "                if not categories:\n",
        "                    cat_elements = item.find_all(['span', 'div'], class_=re.compile('category|type|tag'))\n",
        "                    for cat in cat_elements:\n",
        "                        cat_text = cat.get_text(strip=True)\n",
        "                        if cat_text:\n",
        "                            categories.append(cat_text)\n",
        "                \n",
        "                category = ', '.join(categories) if categories else None\n",
        "                \n",
        "                # Add to list\n",
        "                all_enforcement_actions.append({\n",
        "                    'title': title,\n",
        "                    'date': date_str,\n",
        "                    'category': category,\n",
        "                    'link': link\n",
        "                })\n",
        "            else:\n",
        "                # If we've gone past our target date, we can stop\n",
        "                print(f\"  Reached articles before target date ({date_str}). Stopping.\")\n",
        "                should_continue = False\n",
        "                break\n",
        "        \n",
        "        # If we didn't find anything in range on this page, continue to next page\n",
        "        # Check for next page\n",
        "        pagination = soup.find(['nav', 'div'], class_=re.compile('pag'))\n",
        "        has_next = False\n",
        "        \n",
        "        if pagination:\n",
        "            # Look for next link\n",
        "            next_link = pagination.find('a', text=re.compile(r'Next|›|»|next', re.IGNORECASE))\n",
        "            if next_link and 'disabled' not in next_link.get('class', []):\n",
        "                has_next = True\n",
        "            \n",
        "            # Also check for numbered page links\n",
        "            if not has_next:\n",
        "                page_links = pagination.find_all('a', href=re.compile(r'\\?page=\\d+'))\n",
        "                if page_links:\n",
        "                    has_next = True\n",
        "        \n",
        "        if not has_next:\n",
        "            print(\"  No next page available. Stopping.\")\n",
        "            should_continue = False\n",
        "        \n",
        "        # Increment page number\n",
        "        page_num += 1\n",
        "        \n",
        "        # Safety: don't scrape more than 100 pages\n",
        "        if page_num >= 100:\n",
        "            print(\"  Reached maximum page limit (100). Stopping.\")\n",
        "            should_continue = False\n",
        "        \n",
        "        # Add delay to avoid overwhelming the server\n",
        "        if should_continue:\n",
        "            time.sleep(1)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    if all_enforcement_actions:\n",
        "        df = pd.DataFrame(all_enforcement_actions)\n",
        "        \n",
        "        # Save to CSV\n",
        "        filename = f\"enforcement_actions_{year}_{month}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Scraping complete!\")\n",
        "        print(f\"Total enforcement actions collected: {len(df)}\")\n",
        "        if 'date' in df.columns and not df['date'].isna().all():\n",
        "            print(f\"Date range: {df['date'].iloc[-1]} to {df['date'].iloc[0]}\")\n",
        "        print(f\"Saved to: {filename}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        return df\n",
        "    else:\n",
        "        print(\"No enforcement actions found in the specified date range.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4db9dfbc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting to scrape enforcement actions from 1/2024 to today...\n",
            "Scraping page 1... (URL: https://oig.hhs.gov/fraud/enforcement/)\n",
            "No items found on page 1. Stopping.\n",
            "No enforcement actions found in the specified date range.\n"
          ]
        }
      ],
      "source": [
        "# Test the improved scraper\n",
        "df_2024_v2 = scrape_enforcement_actions_v2(month=1, year=2024, run=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "69d1c2c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Still no data. Let's check the HTML structure manually.\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "if df_2024_v2 is not None and len(df_2024_v2) > 0:\n",
        "    print(\"SUCCESS! Data scraped successfully.\\n\")\n",
        "    print(f\"Total records: {len(df_2024_v2)}\")\n",
        "    print(f\"\\nFirst 5 records:\")\n",
        "    display(df_2024_v2.head())\n",
        "    \n",
        "    print(f\"\\nEarliest enforcement action:\")\n",
        "    earliest = df_2024_v2.iloc[-1]\n",
        "    print(f\"Date: {earliest['date']}\")\n",
        "    print(f\"Title: {earliest['title']}\")\n",
        "    print(f\"Category: {earliest['category']}\")\n",
        "    print(f\"Link: {earliest['link']}\")\n",
        "else:\n",
        "    print(\"Still no data. Let's check the HTML structure manually.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a048df7",
      "metadata": {},
      "source": [
        "## 🔧 CRITICAL: Test the Scraper on First Page Before Running Full Scrape\n",
        "\n",
        "Let's test if the scraper can find items on the first page before running the expensive multi-page scrape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7adc20de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test: Can we scrape the first page at all?\n",
        "print(\"=== TESTING FIRST PAGE SCRAPING ===\\n\")\n",
        "\n",
        "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Test all strategies\n",
        "print(\"Strategy 1 - Looking for <article> tags:\")\n",
        "articles = soup.find_all('article')\n",
        "print(f\"  Found: {len(articles)}\")\n",
        "\n",
        "print(\"\\nStrategy 2 - Looking for divs with 'views-row' class:\")\n",
        "rows = soup.find_all('div', class_=re.compile('views-row'))\n",
        "print(f\"  Found: {len(rows)}\")\n",
        "\n",
        "print(\"\\nStrategy 3 - Looking for main content area:\")\n",
        "main_content = soup.find('div', class_='views-view-content')\n",
        "if main_content:\n",
        "    items = main_content.find_all('div', recursive=False)\n",
        "    print(f\"  Found main content with {len(items)} direct children\")\n",
        "else:\n",
        "    print(\"  Main content not found\")\n",
        "\n",
        "print(\"\\nStrategy 4 - Looking for h2 headings:\")\n",
        "h2_tags = soup.find_all('h2')\n",
        "print(f\"  Found: {len(h2_tags)}\")\n",
        "\n",
        "if h2_tags:\n",
        "    print(f\"\\n  First h2 text: {h2_tags[0].get_text(strip=True)[:80]}\")\n",
        "    print(f\"  First h2 parent: {h2_tags[0].parent.name}\")\n",
        "    print(f\"  First h2 parent class: {h2_tags[0].parent.get('class')}\")\n",
        "\n",
        "# Try to extract one record manually\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MANUAL EXTRACTION TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if h2_tags:\n",
        "    # Try the first h2\n",
        "    first_item = h2_tags[0].parent\n",
        "    \n",
        "    # Try to find link\n",
        "    link_element = first_item.find('a')\n",
        "    if link_element:\n",
        "        title = link_element.get_text(strip=True)\n",
        "        link = link_element.get('href')\n",
        "        print(f\"✅ Title found: {title[:80]}\")\n",
        "        print(f\"✅ Link found: {link}\")\n",
        "    \n",
        "    # Try to find date\n",
        "    date_element = first_item.find('time')\n",
        "    if date_element:\n",
        "        date = date_element.get_text(strip=True)\n",
        "        print(f\"✅ Date found: {date}\")\n",
        "    else:\n",
        "        # Try pattern matching\n",
        "        text = first_item.get_text()\n",
        "        date_pattern = r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}'\n",
        "        match = re.search(date_pattern, text)\n",
        "        if match:\n",
        "            print(f\"✅ Date found (pattern): {match.group(0)}\")\n",
        "        else:\n",
        "            print(\"❌ Date not found\")\n",
        "    \n",
        "    # Try to find categories\n",
        "    categories = first_item.find_all('li')\n",
        "    if categories:\n",
        "        print(f\"✅ Categories found: {[c.get_text(strip=True) for c in categories[:3]]}\")\n",
        "    else:\n",
        "        print(\"❌ Categories not found\")\n",
        "else:\n",
        "    print(\"❌ No h2 tags found - cannot test extraction\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e50b74dc",
      "metadata": {},
      "source": [
        "### 📊 Interpreting the Test Results\n",
        "\n",
        "**What to look for:**\n",
        "\n",
        "1. **If you see \"Found: 0\" for all strategies** → The website might have changed its structure or is blocking scraping\n",
        "   - Solution: Try saving the HTML and inspecting it manually\n",
        "   \n",
        "2. **If one strategy shows > 0 items** → We can use that strategy!\n",
        "   - Look for which strategy worked (has a non-zero count)\n",
        "   - We'll update the scraper to use that specific method\n",
        "   \n",
        "3. **If manual extraction succeeds** → The data is there, we just need to adjust our selectors\n",
        "   - Note which elements were found (✅) and which weren't (❌)\n",
        "   - We'll update the scraper based on this information\n",
        "\n",
        "**Next steps after running the test above:**\n",
        "- Share the output with me so I can see which strategy works\n",
        "- I'll then fix the scraper function to use the correct HTML structure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff8a7cd",
      "metadata": {},
      "source": [
        "If still having issues, save the HTML for manual inspection Uncomment to save HTML\n",
        "\n",
        "with open('enforcement_page.html', 'w', encoding='utf-8') as f:\n",
        "f.write(response.text)\n",
        "print(\"HTML saved to enforcement_page.html for manual inspection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c84870",
      "metadata": {},
      "source": [
        "### Part c: Collect Enforcement Actions Since January 2022\n",
        "\n",
        "Now let's scrape all enforcement actions from January 2022 to today. This will take a while as it needs to go through multiple pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "5182a6a6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting to scrape enforcement actions from 1/2022 to today...\n",
            "Scraping page 1... (URL: https://oig.hhs.gov/fraud/enforcement/)\n",
            "No items found on page 1. Stopping.\n",
            "No enforcement actions found in the specified date range.\n"
          ]
        }
      ],
      "source": [
        "# Scrape enforcement actions from January 2022 to today\n",
        "# This will take several minutes as it needs to process multiple pages\n",
        "df_2022 = scrape_enforcement_actions_v2(month=1, year=2022, run=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f7a2f7bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data collected. Please check the scraping function.\n"
          ]
        }
      ],
      "source": [
        "# Analyze the results from January 2022 scraping\n",
        "if df_2022 is not None and len(df_2022) > 0:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ANALYSIS OF ENFORCEMENT ACTIONS SINCE JANUARY 2022\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Total number of enforcement actions\n",
        "    total_actions = len(df_2022)\n",
        "    print(f\"\\n1. Total enforcement actions collected: {total_actions}\")\n",
        "    \n",
        "    # Find the earliest enforcement action\n",
        "    print(f\"\\n2. Earliest Enforcement Action Details:\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Get the last row (data should be in reverse chronological order)\n",
        "    earliest_action = df_2022.iloc[-1]\n",
        "    \n",
        "    print(f\"   Date: {earliest_action['date']}\")\n",
        "    print(f\"   Title: {earliest_action['title']}\")\n",
        "    print(f\"   Category: {earliest_action['category']}\")\n",
        "    print(f\"   Link: {earliest_action['link']}\")\n",
        "    \n",
        "    # Show the most recent action\n",
        "    print(f\"\\n3. Most Recent Enforcement Action Details:\")\n",
        "    print(\"-\" * 70)\n",
        "    most_recent = df_2022.iloc[0]\n",
        "    print(f\"   Date: {most_recent['date']}\")\n",
        "    print(f\"   Title: {most_recent['title']}\")\n",
        "    print(f\"   Category: {most_recent['category']}\")\n",
        "    print(f\"   Link: {most_recent['link']}\")\n",
        "    \n",
        "    # Show data summary\n",
        "    print(f\"\\n4. Data Summary:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(df_2022.info())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "else:\n",
        "    print(\"No data collected. Please check the scraping function.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1155cf9d",
      "metadata": {},
      "source": [
        "# Step 3: Plot Data Based on Scraped Data\n",
        "\n",
        "For this section, we'll use the CSV file created in Step 2 (enforcement_actions_2022_1.csv) to create visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "07fd2ed1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: No data available. Please run the scraper first.\n"
          ]
        }
      ],
      "source": [
        "# Import visualization libraries\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "\n",
        "# Load the data from CSV (if not already in memory)\n",
        "try:\n",
        "    df = pd.read_csv('enforcement_actions_2022_1.csv')\n",
        "    print(f\"Loaded {len(df)} records from CSV file\")\n",
        "except FileNotFoundError:\n",
        "    # If CSV doesn't exist, use the dataframe from memory\n",
        "    if df_2022 is not None:\n",
        "        df = df_2022.copy()\n",
        "        print(f\"Using {len(df)} records from memory\")\n",
        "    else:\n",
        "        print(\"Error: No data available. Please run the scraper first.\")\n",
        "        df = None\n",
        "\n",
        "if df is not None:\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3d972e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing: Parse dates and create month-year column\n",
        "if df is not None:\n",
        "    # Parse date column to datetime\n",
        "    df['date_parsed'] = pd.to_datetime(df['date'], format='%B %d, %Y', errors='coerce')\n",
        "    \n",
        "    # Create month-year column for aggregation\n",
        "    df['month_year'] = df['date_parsed'].dt.to_period('M')\n",
        "    \n",
        "    # Sort by date\n",
        "    df = df.sort_values('date_parsed')\n",
        "    \n",
        "    print(\"Data preprocessing complete!\")\n",
        "    print(f\"\\nDate range: {df['date_parsed'].min()} to {df['date_parsed'].max()}\")\n",
        "    print(f\"\\nSample of processed data:\")\n",
        "    display(df[['date', 'date_parsed', 'month_year', 'title']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9203f2d2",
      "metadata": {},
      "source": [
        "## Plot 1: Number of Enforcement Actions Over Time\n",
        "\n",
        "Line chart showing the number of enforcement actions aggregated by month+year overall since January 2022."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3b2683",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate data by month-year\n",
        "if df is not None:\n",
        "    # Count enforcement actions by month\n",
        "    monthly_counts = df.groupby('month_year').size().reset_index(name='count')\n",
        "    \n",
        "    # Convert period to string for Altair\n",
        "    monthly_counts['month_year_str'] = monthly_counts['month_year'].astype(str)\n",
        "    \n",
        "    print(\"Monthly aggregation:\")\n",
        "    display(monthly_counts.head(10))\n",
        "    \n",
        "    # Create line chart with Altair\n",
        "    chart1 = alt.Chart(monthly_counts).mark_line(point=True).encode(\n",
        "        x=alt.X('month_year_str:T', \n",
        "                title='Month-Year',\n",
        "                axis=alt.Axis(labelAngle=-45)),\n",
        "        y=alt.Y('count:Q', \n",
        "                title='Number of Enforcement Actions'),\n",
        "        tooltip=['month_year_str:T', 'count:Q']\n",
        "    ).properties(\n",
        "        title='Number of Enforcement Actions Over Time (January 2022 - Present)',\n",
        "        width=700,\n",
        "        height=400\n",
        "    ).configure_axis(\n",
        "        labelFontSize=12,\n",
        "        titleFontSize=14\n",
        "    ).configure_title(\n",
        "        fontSize=16\n",
        "    )\n",
        "    \n",
        "    chart1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56afced7",
      "metadata": {},
      "source": [
        "## Plot 2: Enforcement Actions by Category\n",
        "\n",
        "We need to create categories based on the enforcement action types:\n",
        "1. \"Criminal and Civil Actions\" vs. \"State Enforcement Agencies\"\n",
        "2. Within \"Criminal and Civil Actions\", classify into 5 topics based on title keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "f2e20267",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classify enforcement actions by main category\n",
        "if df is not None:\n",
        "    # First, determine if it's \"Criminal and Civil Actions\" or \"State Enforcement Agencies\"\n",
        "    df['main_category'] = df['category'].apply(\n",
        "        lambda x: 'State Enforcement Agencies' if pd.notna(x) and 'State Enforcement' in str(x) \n",
        "        else 'Criminal and Civil Actions'\n",
        "    )\n",
        "    \n",
        "    print(\"Main category distribution:\")\n",
        "    print(df['main_category'].value_counts())\n",
        "    print(\"\\nSample of categorized data:\")\n",
        "    display(df[['title', 'category', 'main_category']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "28d5c3d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to classify Criminal and Civil Actions into 5 topics based on title\n",
        "def classify_criminal_civil_topic(title):\n",
        "    \"\"\"\n",
        "    Classify enforcement actions into 5 topics based on keywords in the title.\n",
        "    Topics: Health Care Fraud, Financial Fraud, Drug Enforcement, Bribery/Corruption, Other\n",
        "    \"\"\"\n",
        "    if pd.isna(title):\n",
        "        return 'Other'\n",
        "    \n",
        "    title_lower = title.lower()\n",
        "    \n",
        "    # Health Care Fraud keywords\n",
        "    health_keywords = ['health care', 'healthcare', 'medicare', 'medicaid', 'hospital', \n",
        "                       'doctor', 'physician', 'medical', 'clinic', 'prescription',\n",
        "                       'patient', 'nursing', 'pharmacy', 'drug test', 'diagnostic',\n",
        "                       'therapy', 'treatment', 'insurance claim', 'billing']\n",
        "    \n",
        "    # Financial Fraud keywords\n",
        "    financial_keywords = ['bank', 'financial', 'money laundering', 'wire fraud', \n",
        "                         'securities', 'investment', 'ponzi', 'embezzle', \n",
        "                         'tax fraud', 'loan fraud', 'credit card', 'mortgage']\n",
        "    \n",
        "    # Drug Enforcement keywords\n",
        "    drug_keywords = ['drug', 'opioid', 'fentanyl', 'controlled substance', \n",
        "                    'narcotic', 'cocaine', 'heroin', 'methamphetamine',\n",
        "                    'prescription drug', 'illegal distribution', 'dea']\n",
        "    \n",
        "    # Bribery/Corruption keywords\n",
        "    bribery_keywords = ['bribery', 'bribe', 'kickback', 'corruption', 'corrupt',\n",
        "                       'conspiracy', 'illegal payment', 'unlawful payment']\n",
        "    \n",
        "    # Check keywords in order of specificity\n",
        "    # Check bribery first (often mentioned in titles)\n",
        "    if any(keyword in title_lower for keyword in bribery_keywords):\n",
        "        return 'Bribery/Corruption'\n",
        "    \n",
        "    # Check drug enforcement\n",
        "    if any(keyword in title_lower for keyword in drug_keywords):\n",
        "        return 'Drug Enforcement'\n",
        "    \n",
        "    # Check financial fraud\n",
        "    if any(keyword in title_lower for keyword in financial_keywords):\n",
        "        return 'Financial Fraud'\n",
        "    \n",
        "    # Check health care fraud (most common, check last to avoid over-classification)\n",
        "    if any(keyword in title_lower for keyword in health_keywords):\n",
        "        return 'Health Care Fraud'\n",
        "    \n",
        "    # Default to Other\n",
        "    return 'Other'\n",
        "\n",
        "# Apply classification to Criminal and Civil Actions only\n",
        "if df is not None:\n",
        "    df['topic'] = df.apply(\n",
        "        lambda row: classify_criminal_civil_topic(row['title']) \n",
        "        if row['main_category'] == 'Criminal and Civil Actions' \n",
        "        else row['main_category'],\n",
        "        axis=1\n",
        "    )\n",
        "    \n",
        "    print(\"Topic distribution within Criminal and Civil Actions:\")\n",
        "    criminal_civil = df[df['main_category'] == 'Criminal and Civil Actions']\n",
        "    print(criminal_civil['topic'].value_counts())\n",
        "    \n",
        "    print(\"\\n\\nOverall topic distribution:\")\n",
        "    print(df['topic'].value_counts())\n",
        "    \n",
        "    print(\"\\n\\nSample classifications:\")\n",
        "    display(df[['title', 'main_category', 'topic']].head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c812182",
      "metadata": {},
      "source": [
        "### Plot 2a: Criminal and Civil Actions vs. State Enforcement Agencies Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "b76239bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate by month and main category\n",
        "if df is not None:\n",
        "    monthly_category = df.groupby(['month_year', 'main_category']).size().reset_index(name='count')\n",
        "    monthly_category['month_year_str'] = monthly_category['month_year'].astype(str)\n",
        "    \n",
        "    print(\"Monthly counts by main category:\")\n",
        "    display(monthly_category.head(10))\n",
        "    \n",
        "    # Create line chart\n",
        "    chart2a = alt.Chart(monthly_category).mark_line(point=True).encode(\n",
        "        x=alt.X('month_year_str:T', \n",
        "                title='Month-Year',\n",
        "                axis=alt.Axis(labelAngle=-45)),\n",
        "        y=alt.Y('count:Q', \n",
        "                title='Number of Enforcement Actions'),\n",
        "        color=alt.Color('main_category:N', \n",
        "                       title='Category',\n",
        "                       scale=alt.Scale(scheme='category10')),\n",
        "        tooltip=['month_year_str:T', 'main_category:N', 'count:Q']\n",
        "    ).properties(\n",
        "        title='Enforcement Actions: Criminal and Civil vs. State Agencies',\n",
        "        width=700,\n",
        "        height=400\n",
        "    ).configure_axis(\n",
        "        labelFontSize=12,\n",
        "        titleFontSize=14\n",
        "    ).configure_title(\n",
        "        fontSize=16\n",
        "    )\n",
        "    \n",
        "    chart2a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eabd6c1",
      "metadata": {},
      "source": [
        "### Plot 2b: Five Topics within Criminal and Civil Actions Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5e3d2b5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for Criminal and Civil Actions only and aggregate by topic\n",
        "if df is not None:\n",
        "    criminal_civil_df = df[df['main_category'] == 'Criminal and Civil Actions'].copy()\n",
        "    \n",
        "    monthly_topic = criminal_civil_df.groupby(['month_year', 'topic']).size().reset_index(name='count')\n",
        "    monthly_topic['month_year_str'] = monthly_topic['month_year'].astype(str)\n",
        "    \n",
        "    print(\"Monthly counts by topic (Criminal and Civil Actions only):\")\n",
        "    display(monthly_topic.head(15))\n",
        "    \n",
        "    # Create line chart\n",
        "    chart2b = alt.Chart(monthly_topic).mark_line(point=True).encode(\n",
        "        x=alt.X('month_year_str:T', \n",
        "                title='Month-Year',\n",
        "                axis=alt.Axis(labelAngle=-45)),\n",
        "        y=alt.Y('count:Q', \n",
        "                title='Number of Enforcement Actions'),\n",
        "        color=alt.Color('topic:N', \n",
        "                       title='Topic',\n",
        "                       scale=alt.Scale(scheme='category10')),\n",
        "        tooltip=['month_year_str:T', 'topic:N', 'count:Q']\n",
        "    ).properties(\n",
        "        title='Criminal and Civil Actions by Topic Over Time',\n",
        "        width=700,\n",
        "        height=400\n",
        "    ).configure_axis(\n",
        "        labelFontSize=12,\n",
        "        titleFontSize=14\n",
        "    ).configure_title(\n",
        "        fontSize=16\n",
        "    ).configure_legend(\n",
        "        labelFontSize=11\n",
        "    )\n",
        "    \n",
        "    chart2b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b12a83",
      "metadata": {},
      "source": [
        "## Summary Statistics\n",
        "\n",
        "Let's look at some summary statistics for the scraped data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "fd3731f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "if df is not None:\n",
        "    print(\"=\"*70)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(f\"\\n1. Total Enforcement Actions: {len(df)}\")\n",
        "    \n",
        "    print(f\"\\n2. Date Range:\")\n",
        "    print(f\"   Earliest: {df['date_parsed'].min()}\")\n",
        "    print(f\"   Latest: {df['date_parsed'].max()}\")\n",
        "    \n",
        "    print(f\"\\n3. Main Category Distribution:\")\n",
        "    print(df['main_category'].value_counts())\n",
        "    \n",
        "    print(f\"\\n4. Topic Distribution (Criminal and Civil Actions):\")\n",
        "    criminal_civil = df[df['main_category'] == 'Criminal and Civil Actions']\n",
        "    print(criminal_civil['topic'].value_counts())\n",
        "    \n",
        "    print(f\"\\n5. Average Enforcement Actions per Month:\")\n",
        "    avg_per_month = df.groupby('month_year').size().mean()\n",
        "    print(f\"   {avg_per_month:.1f} actions/month\")\n",
        "    \n",
        "    print(f\"\\n6. Month with Most Enforcement Actions:\")\n",
        "    max_month = df.groupby('month_year').size().idxmax()\n",
        "    max_count = df.groupby('month_year').size().max()\n",
        "    print(f\"   {max_month}: {max_count} actions\")\n",
        "    \n",
        "    print(f\"\\n7. Month with Least Enforcement Actions:\")\n",
        "    min_month = df.groupby('month_year').size().idxmin()\n",
        "    min_count = df.groupby('month_year').size().min()\n",
        "    print(f\"   {min_month}: {min_count} actions\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29971747",
      "metadata": {},
      "source": [
        "## Explanation of Classification Methodology\n",
        "\n",
        "### How the Topic Classification Works:\n",
        "\n",
        "The classification system uses keyword matching to categorize enforcement actions into 5 topics:\n",
        "\n",
        "1. **Health Care Fraud**: Keywords include \"health care\", \"medicare\", \"medicaid\", \"hospital\", \"doctor\", \"medical\", etc.\n",
        "   - Example: \"Hospital Executive Pleads Guilty to Medicare Fraud Scheme\"\n",
        "\n",
        "2. **Financial Fraud**: Keywords include \"bank\", \"financial\", \"money laundering\", \"wire fraud\", \"securities\", etc.\n",
        "   - Example: \"Brooklyn Banker Pleads Guilty to Laundering Proceeds\"\n",
        "\n",
        "3. **Drug Enforcement**: Keywords include \"drug\", \"opioid\", \"fentanyl\", \"controlled substance\", \"narcotic\", etc.\n",
        "   - Example: \"Pharmacy Owner Charged with Illegal Distribution of Opioids\"\n",
        "\n",
        "4. **Bribery/Corruption**: Keywords include \"bribery\", \"kickback\", \"corruption\", \"conspiracy\", etc.\n",
        "   - Example: \"Official Sentenced for Accepting Kickbacks\"\n",
        "\n",
        "5. **Other**: Any enforcement action that doesn't match the above categories\n",
        "   - Example: General fraud cases not fitting other categories\n",
        "\n",
        "The classification prioritizes specificity, checking for bribery/corruption and drug enforcement first (as these are often explicitly mentioned), then financial fraud, and finally health care fraud (which is the most common category)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
